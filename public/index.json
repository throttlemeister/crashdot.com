
[{"content":"","date":"9 November 2024","externalUrl":null,"permalink":"/","section":"crashdot.com","summary":"","title":"crashdot.com","type":"page"},{"content":"","date":"9 November 2024","externalUrl":null,"permalink":"/tags/linux/","section":"Tags","summary":"","title":"Linux","type":"tags"},{"content":"","date":"9 November 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":"","date":"9 November 2024","externalUrl":null,"permalink":"/tags/terminal/","section":"Tags","summary":"","title":"Terminal","type":"tags"},{"content":" Introduction # Over time, I have collected or installed a lot of tools and utilities that make my life in the terminal a little easier. Some help me visualize information on the terminal, others just simply make life easier by letting me achieve my goal quicker. Whatever it is, most of these I could not live without anymore. So here is a list of utilities I use and love.\nFish # Fish, the friendly shell. This has to come first. For most of the tools after this, I created little fish functions to execute them as I want, and I will include them in their respective sections as well.\nFish allows for easy scripting of complex functions. It is extremely functional right out of the box. It has an abbreviation system that lets you quickly and intuitively shorten often used commands so you don\u0026rsquo;t have to type them out each and every time. It even has a web-based configuration utility that you can use to graphically configure it to your liking.\nBut for me, the ease with which you can write your own functions and extend its functionality if king here.\nHomepage: Fish\neza # eza is a modern, maintained replacement for the venerable file-listing command-line program ls that ships with Unix and Linux operating systems, giving it more features and better defaults. It uses colours to distinguish file types and metadata. It knows about symlinks, extended attributes, and Git. And it’s small, fast, and just one single binary.\nBy deliberately making some decisions differently, eza attempts to be a more featureful, more user-friendly version of ls.\nThe way I use it in fish:\nfunction ls -d 'eza instead of ls' if type --quiet eza \u0026amp;\u0026amp; test \u0026quot;$argv[1]\u0026quot; != \u0026quot;-ltr\u0026quot; eza --header --group-directories-first --git --icons=auto $argv else command ls --color=always $argv end end function ll if command -sq eza ls -laa -g $argv else command ls -la $argv end end function l ls $argv end function lt if command -sq eza ls -laa -snew -g $argv; else command ls -ltr $argv; end end Homepage: eza\nripgrep # Ripgrep is a replacement for the standard grep command. It can search recursively, respects .gitignore and skips hidden files or directories and binary files.\n# Defined in - @ line 1 # # Let's check if we have ripgrep installed and if so # we use it, otherwise use standard grep with options function grep if command -sq rg rg $argv; else command grep -n --color $argv; end end Homepage: Ripgrep\nbat # Bat is a, as the homepage tell you themselves, a \u0026ldquo;cat clone with wings\u0026rdquo;. It supports syntax highlighting, it shows linenumbers, it has git integration, it shows non-printable characters and it automatically paginates the output. This makes it a lot more usable and readable than the regular cat, which just outputs flat ascii.\nfunction cat -d 'bat instead of cat' if type --quiet bat bat $argv else command cat $argv end end Homepage: Bat\nzoxide # Zoxide is a smarter cd command and my new best friend. It takes inspiration from z and autojump and the one cool thing it does is that it keeps a database of the directories you visit. You can then change directory by just typing the directory or part thereof, and it jumps right in no matter how deep or your current location. Also a simple cd - jumps you back to where you came from, making switching back and forth really quick. I don\u0026rsquo;t think I can live without this little utility anymore.\nfunction cd -d 'Using zoxide as replacement for cd, if it exists' if command -sq zoxide z $argv else cd $argv end end Homepage: zoxide\nadvcp \u0026amp; advmv # Advanced Copy is a mod for the GNU cp and GNU mv tools which adds a progress bar and provides some info on what\u0026rsquo;s going on. There is no real difference to the standard cp and mv commands, but I do very much appreciate that it gives feedback on progress. Why isn\u0026rsquo;t this included by default?\nIn my fish config:\nfunction cp if command -sq advcp advcp -g $argv else command cp $argv end end (same for mv)\nHomepage: advcp/mv\nstow # From their homepage:\nGNU Stow is a symlink farm manager which takes distinct packages of software and/or data located in separate directories on the filesystem, and makes them appear to be installed in the same place. For example, /usr/local/bin could contain symlinks to files within /usr/local/stow/emacs/bin, /usr/local/stow/perl/bin etc., and likewise recursively for any other subdirectories such as \u0026hellip;/share, \u0026hellip;/man, and so on.\nI like and use it to manage my dotfiles in a simple, intuitive way. I actually wrote an article about it, which you can read here.\n# Function for stow, to ingore .directory files and use 'dotfiles' special handling functionality # function dotf -d \u0026quot;Use stow with extra parameters\u0026quot; if command -sq stow stow -d ~/.dotfiles/ $argv --ignore=.directory --ignore=README.md --dotfiles else echo \u0026quot;Stow not installed. Please install before using.\u0026quot; end end Homepage: GNU stow\nfastfetch # Admit it, everybody loves neofetch. However, neofetch is no longer maintained and fastfetch does the same thing. Except it is written in C and as such it is blistering fast. It is also highly configurabla, just like its cousin.\nfunction ff -d 'fastfetch shortcut' if type --quiet fastfetch if test -n \u0026quot;$ALACRITTY_WINDOW_ID\u0026quot; fastfetch -l opensuse -c examples/6.jsonc $argv else fastfetch -l ~/ansible/files/twgrey.png --logo-type iterm --logo-padding-top 2 --logo-width 45 -c examples/6.jsonc $argv end else command neofetch $argv end end I do a check on alacritty there, as it does not seem to like me using an image file for the distro logo. So I revert to a regular ASCII logo if it detects alacritty as the terminal emulator.\nHomepage: fastfetch\nneovim # Neovim is a hyperextensible vim-based editor. It is a drop-in for replacement for vim and is customizable to the extreme. I use it in combination with lazyvim, which is a plugin manager and allows for easy configuration and customization.\nfunction vi --wraps=\u0026quot;neovim to vi\u0026quot; if command -sq nvim nvim $argv else command vi $argv end end Homepage: neovim Homepage: LazyVim\n","date":"9 November 2024","externalUrl":null,"permalink":"/posts/linux/utils/","section":"Articles","summary":"Introduction # Over time, I have collected or installed a lot of tools and utilities that make my life in the terminal a little easier.","title":"Top Utilities and Tools","type":"posts"},{"content":"","date":"9 November 2024","externalUrl":null,"permalink":"/tags/utilities/","section":"Tags","summary":"","title":"Utilities","type":"tags"},{"content":" Introduction # Well, here we are. A year after installing OpenSUSE Tumbleweed (actually, 14 months already) and we are still running it without much complaints, if any. Solid, fast, what more is there to ask for?\nThings I have noticed # Over this time period I have noticed some Things.\nPro:\nAlways the latest software It does not fail, despite being a rolling release distribution Easy to configure and customize (\u0026lsquo;ricing\u0026rsquo;) YaST let\u0026rsquo;s you configure certaint things quicker and more reliable than you could on the commandline (iSCSI, NFS mounts) Neg:\nSometimes (rarely) an update breaks something Codecs are not in the main repos due to licensing and Packman repos are sometimes out of sync, forcing you to wait for specific updates Typically this is not more than a few days, sometimes up to a week Migration from nvidia to AMD could have been smoother, but not really something Tumbleweed is at fault for. By default qgroups are enabled on BTRFS, which is good for snapshot management but can cause some performance issues. Summing up # Overall, very happy still. I wouldn\u0026rsquo;t know what distribution to run that would make me more content running Liux on a day to day basis, really. It\u0026rsquo;s a solid performer and with a nice balances of running the latest and greatest without too much risk of annoyances due to the proper testing that is done by OBS. I do look forward to trying out the new Cosmic desktop from System76 when it is released and made available, but I doubt I would be leaving my trusty and loved KDE behind.\n","date":"30 October 2024","externalUrl":null,"permalink":"/posts/opensuse/12months/","section":"Articles","summary":"Introduction # Well, here we are.","title":"One year of Tumbleweed","type":"posts"},{"content":"","date":"30 October 2024","externalUrl":null,"permalink":"/tags/opensuse/","section":"Tags","summary":"","title":"Opensuse","type":"tags"},{"content":" Introduction # If you have been here before, you will have noticed some changes. Previously, everything I posted was on sport-touring.eu and hosted by a WordPress site locally. This is no longer the case.\nWhy the changes? # First and foremost, WordPress was giving me headaches. While it is an impressive suite, it is also a bit fragile at times. Recently, I was getting errors wanting to click to articles and couldn\u0026rsquo;t really find what was wrong. Granted, I didn\u0026rsquo;t look very long, but still. It shouldn\u0026rsquo;t been happening as nothing changed.\nNow what? # A while back, I had heard of Hugo. I didn\u0026rsquo;t do much with it, just installed it and played around with it shortly. It looked interesting enough, but I didn\u0026rsquo;t know enough Markdown formatting to be really useful with it.\nWhat is Hugo? # Hugo is one of the most popular open-source static site generators. With its amazing speed and flexibility, Hugo makes building websites fun again.\nThis is direct quote from their website. And that is exactly it.\nOk\u0026hellip; # Hugo does have a bit of a learning curve, as you need to write your content in Markdown. But it is editing in flat text format. When you are done, you just run Hugo and it will generate static html pages for your entire site, resulting in a blazing fast experience and no bloat.\nHaving said that, it does make things easier and takes the complexity out of your site without having to sacrifice features or looks. There are plenty of themes you can install and modify and when you have the look that you want, you don\u0026rsquo;t have to touch it anymore. It just gets rendered with your content.\nAlso, hosting has changed. Previously my site was running on my NAS. That\u0026rsquo;s perfectly fine. I have 1Gbit/1Gbit connection and reliable storage. Then again, Hugo let\u0026rsquo;s me integrate with github. Not just for version control, but you can make use of github actions (github even has a default action for Hugo), that let\u0026rsquo;s you bring your site live, on github, after commit. Even with a custom domain. Pretty darn cool if you ask me. And Hugo let\u0026rsquo;s you preview your site in your browser locally, so you can just work on it and only commit when you are happy. So I moved of my NAS and onto github for my sites so i can edit whereever I am, whenever.\nConclusion # So that\u0026rsquo;s it. Back to plain old html, no fuss and automated deployments. I do still have a lot to learn, but that is just part of the fun. I hope you are enjoying the content as much as I am working on it.\n","date":"27 October 2024","externalUrl":null,"permalink":"/posts/stuff/changes/","section":"Articles","summary":"Introduction # If you have been here before, you will have noticed some changes.","title":"changes","type":"posts"},{"content":"","date":"27 October 2024","externalUrl":null,"permalink":"/tags/hugo/","section":"Tags","summary":"","title":"Hugo","type":"tags"},{"content":"","date":"27 October 2024","externalUrl":null,"permalink":"/tags/various/","section":"Tags","summary":"","title":"Various","type":"tags"},{"content":"","date":"17 September 2024","externalUrl":null,"permalink":"/tags/dotfiles/","section":"Tags","summary":"","title":"Dotfiles","type":"tags"},{"content":" Intro # One thing that’s always a bit of a pain when you use multiple computers, or when you want to reinstall your computer at some point in time is that you have to recreate your configuration files or ‘dotfiles’ to have everything back as you want again.\nStow # There are several ways of doing this, but recently I came across a little tool called GNU stow. It is a much easier way of managing dotfiles than for instance using a git bare repository.\nHow stow works, is you create “stow directory”, in my case dotfiles. Inside that directory you can create further directories for “packages”. For this use, a “package” is basically a collection of files that belong together. Example: a bash directory would store all your bash configuration files like .bashrc, .bash_fucntions, .profile, etc..\nPutting it together # Once you have your “package” directory, you can move your dotfiles for that “package” into that directory. (move, so they are no longer in their normal location). If you now call stow with the package name, for instance stow bash it will create symbolic links from all files to their original location.\nIf you have files under ~/.config then you will need to create a .config directory under your package directory, and then move the files or directories that contain your files under there.\nFor instance, I have a “package” fish, which has the following structure:\n~/.dotfiles/fish/.config/fish And under ~/.config you will find a symlink called fish to that directory.\nSo the structure is basically this:\nStow directory (ie, dotfiles location)\nPackage directory files relative from your home directory directories relative from your home directory files relative from previous directory directories relative from previous directory Running stow on a package will link to files and / or directories starting from your home directory down. If a subdirectory is put in there, like the bottom one above, it will use the highest common directory entry to link to.\nOnce you have your structure set up under your stow directory, you can simply go into your stow directory and issue the command stow * or stow packagename and all links will be created.\nIf you then add your stow directory to a git repository, all you would have to do on a new system or when setting up after install is to clone that repository, make sure you have stow installed and issue the command above to have your setup restored.\nThe easy life in fish shell # Finally, as a little bonus, I created a little fish shell function because I am lazy and don’t want to cd back and forth all the time to make it easier for me to create the links for a “package”. I use fish shell, but the example below should be easy enough to refactor to bash or zsh functions if you use those.\nTo create the links for configuration files for a \u0026lsquo;package\u0026rsquo;:\nfunction add_dot --wraps=\u0026quot;stow\u0026quot; if [ -z $argv ] echo \u0026quot;No argument given; exiting\u0026quot; else set _oldpath $PWD cd ~/.dotfiles stow $argv cd $_oldpath end end Example: to set up the links for your bash dotfiles, just type add_dot bash\nTo remove the links to configuration files for a \u0026lsquo;package\u0026rsquo;:\nfunction rm_dot --wraps=\u0026quot;stow\u0026quot; if [ -z $argv ] echo \u0026quot;No argument given; exiting\u0026quot; else set _oldpath $PWD cd ~/.dotfiles stow -D $argv cd $_oldpath end end Example: to remove the links for your bash dotfiles, just type rm_dot bash\nConclusion # In conclusion, it take a little bit to set up but once that is done, it is very easy to maintain and setting up your profile configuration becomes so easy.\nExample layout of my dotfiles directory:\nalacritty: Permissions Size User Date Modified Git Name .rw-r--r-- 355 throttlemeister 2 Aug 23:26 -- .alacritty.toml bash: Permissions Size User Date Modified Git Name .rw-r--r-- 1.0k throttlemeister 11 Jun 2023 -- .bash_aliases .rw-r--r-- 1.1k throttlemeister 11 Jun 2023 -- .bash_functions .rw-r--r-- 220 throttlemeister 11 Jun 2023 -- .bash_logout .rw-r--r-- 92 throttlemeister 11 Jun 2023 -- .bash_profile .rw-r--r-- 4.3k throttlemeister 11 Jun 2023 -- .bashrc .rwxr-xr-x 30 throttlemeister 11 Jun 2023 -- .inputrc .rw-r--r-- 723 throttlemeister 11 Jun 2023 -- .profile conky: Permissions Size User Date Modified Git Name .rw-r--r-- 11k throttlemeister 13 Aug 23:32 -- .conkyrc fish: Permissions Size User Date Modified Git Name drwxr-xr-x - throttlemeister 17 Sep 14:25 -- .config git: Permissions Size User Date Modified Git Name .rw-r--r-- 166 throttlemeister 17 Sep 16:48 -- .gitconfig .rw-r--r-- 4 throttlemeister 11 Jun 2023 -- .gitignore zsh: Permissions Size User Date Modified Git Name .rw-r--r-- 4.6k throttlemeister 2 Aug 23:15 -- .zshrc .rw-r--r-- 374 throttlemeister 2 Aug 22:45 -- .zshrc.pre-oh-my-zsh ","date":"17 September 2024","externalUrl":null,"permalink":"/posts/linux/dotfiles/","section":"Articles","summary":"Intro # One thing that’s always a bit of a pain when you use multiple computers, or when you want to reinstall your computer at some point in time is that you have to recreate your configuration files or ‘dotfiles’ to have everything back as you want again.","title":"Managing dotfiles using stow on Linux","type":"posts"},{"content":"","date":"2 May 2024","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","date":"2 May 2024","externalUrl":null,"permalink":"/categories/computer/","section":"Categories","summary":"","title":"Computer","type":"categories"},{"content":"","date":"2 May 2024","externalUrl":null,"permalink":"/categories/desktop/","section":"Categories","summary":"","title":"Desktop","type":"categories"},{"content":"","date":"2 May 2024","externalUrl":null,"permalink":"/tags/kde/","section":"Tags","summary":"","title":"Kde","type":"tags"},{"content":"","date":"2 May 2024","externalUrl":null,"permalink":"/categories/kde/","section":"Categories","summary":"","title":"KDE","type":"categories"},{"content":" Introduction # About a month ago, KDE released their much anticipated next major release of the KDE desktop environment, version 6. The only distributions using it soon after were (obviously) KDE Neon, the KDE showcase / development platform, and the rolling release distributions like Arch and OpenSUSE Tumbleweed. The scheduled release distributions will only ship KDE6 in their next new release.\nAs it was released, obviously there were some teething problems despite a long and extensive beta test cycle and lots of bigfixing. Some of these were:\nIssues # Upgrade being interrupted if done from the GUI, as the session was killed, halting the upgrade process. Easy fix: just restart the upgrade and it will finish. If doing a reboot, because you didn\u0026rsquo;t notice there was a problem, you would end up without a GUI. The best way to upgrade, was to switch to a TTY screen and upgade from the CLI without a GUI running. Sometimes, especially with a custom SDDM theme selected, the SDDM screen would fall back to a very ugly UI and an error. This was not critical and setting SDDM to the default breeze theme typicall fixed this. If you used custom themes, plasmoids, plasma addons etc, these could potentially be incompatible with KDE6 and refuse to load. This is annoying but to be expected. Just unload and remove or upgrade. Currently most have been upgraded to work with KDE6, provided they are actively maintained. The default session was changed from X11 to Wayland for all users, including those with Nvidia hardware. In some cases this could cause some issues with Wayland not loading but X11 would generally still work. For me # For me personally, the experience has been great despite experiencing the first 3 from the above list. They were annoying when it happened but easily fixed. Since the experience has been smooth, my addons have been upgraded by their maintainers and the Wayland experience has been very smooth. I\u0026rsquo;m not the biggest fan of Wayland myself (perhaps I will write about that at some point), but it is the future and having to live with it like it is now with KDE6 is not punishment.\nKDE Plasma 6 has been a great experience for me. It has sane defaults. The tweaks in certain settings and places are very good example: the panel configuration, thank you KDE team now you don\u0026rsquo;t need a degree to decipher what some of the things mean.\nIt\u0026rsquo;s more of an evolution than a revolution going from KDE5 to KDE6, but what an evolution it is. The best desktop just got better.. And smoother. And more polished.\n","date":"2 May 2024","externalUrl":null,"permalink":"/posts/linux/kde6/","section":"Articles","summary":"Introduction # About a month ago, KDE released their much anticipated next major release of the KDE desktop environment, version 6.","title":"KDE Plasma 6: Don’t be fooled by the naysayers on the internet","type":"posts"},{"content":"","date":"2 May 2024","externalUrl":null,"permalink":"/tags/kde6/","section":"Tags","summary":"","title":"Kde6","type":"tags"},{"content":"","date":"2 May 2024","externalUrl":null,"permalink":"/categories/linux/","section":"Categories","summary":"","title":"Linux","type":"categories"},{"content":"","date":"23 March 2024","externalUrl":null,"permalink":"/tags/atomic/","section":"Tags","summary":"","title":"Atomic","type":"tags"},{"content":" Introduction # It’s possible to use transactional or atomic updates on OpenSUSE Tumbleweed and leveraging the snapshot capabilities of btrfs. It’s actually quite simple; all you need to do is to install the command using zipper.\nHow # Important prerequisite: the filesystem must be btrfs.\nsudo zypper in transactional-update This will install the command. Next you want to set up some, to prevent some potential issues.\n# Note: this systemd service is required to create RPM dirs in /var as TU # does not mount /var in the new snapshot to maintain atomicity. systemctl enable --now create-dirs-from-rpmdb.service That’s basically it. Now if you run the command:\nsudo transactional-update The system will create a new snapshot, check for updates, install updates on that snapshot and set that snapshot to boot. If there are no updates, the snapshot is removed again.\nKeep in mind, and this is important: any changes you make to the root filesystem will be gone after you reboot into the new snapshot.\nThis is expected behavior, but it does mean if you want or need to make changes you need to make sure you reboot first.\nConclusion # Why would you want this? If you need to apply a big update and you don’t want it to interrupt your work or avoid processes being restarted, you can apply the update on a new snapshot without it interfering or making changes in any way on the running system.\nIs it necessary? Absolutely not. It is just another tool in the toolbox. It offers some of the advantages (atomic updates) of an immutable distribution, without having to have an immutable (read-only) filesystem.\n","date":"23 March 2024","externalUrl":null,"permalink":"/posts/opensuse/transactional/","section":"Articles","summary":"Introduction # It’s possible to use transactional or atomic updates on OpenSUSE Tumbleweed and leveraging the snapshot capabilities of btrfs.","title":"Transactional updates on OpenSUSE Tumbleweed","type":"posts"},{"content":"","date":"2 March 2024","externalUrl":null,"permalink":"/categories/opensuse/","section":"Categories","summary":"","title":"OpenSUSE","type":"categories"},{"content":"Well, here we are, 6 months after installing OpenSUSE Tumbleweed and still no urge to change. The only change I am longing for is the update to KDE Plasma 6, which should be coming soon now that it has officially been released.\nSince there are basically no updates since my last post in December, there isn\u0026rsquo;t much to say. I did however made some changes. A couple of weeks ago, my GTX1080 decided to shit itself such that it triggers the short protection in the power supply so that\u0026rsquo;s out. Bit of a pain since everything is water cooled. I ended up replacing it with a Sapphire R9 390 Nitro which is an AMD based card. Raw speed is about half of that of the GTX1080, but at 50 euros it was cheap and being AMD means no more Nvidia issues on Linux as limited as they were for me. Main issue for me was that I had to manually recreate the initrd whenever a new driver version was released to make it work.\nRouting of the watercooling soft tubing looks like crap now, but hey, its working again and at least I can use my machine again.\nChanging from Nvidia to AMD was a bit of a pain as I had to remove all Nvidia related packages and I had to manually set the correct kernel parameters to make it work. After that it was painless and working just fine. Drivers are part of the kernel, so no more issues there. Test benchmarks suits show lower fps as to be expected, but also smoother and less jerky graphics. I don\u0026rsquo;t game that much, so I guess I\u0026rsquo;ll be fine for now. Would love me some 6700XT or something.\nTumbleweed keeps on rocking.\n","date":"2 March 2024","externalUrl":null,"permalink":"/posts/opensuse/6months/","section":"Articles","summary":"Well, here we are, 6 months after installing OpenSUSE Tumbleweed and still no urge to change.","title":"OpenSUSE Tumbleweed, 6 months in","type":"posts"},{"content":"","date":"18 January 2024","externalUrl":null,"permalink":"/tags/sdboot/","section":"Tags","summary":"","title":"Sdboot","type":"tags"},{"content":"","date":"18 January 2024","externalUrl":null,"permalink":"/tags/systemd-boot/","section":"Tags","summary":"","title":"Systemd-Boot","type":"tags"},{"content":" Introduction # I am weird sometimes, I know. Ever since I used Pop!OS, which uses systemd-boot as default boot manager, I have fallen in love with it. The simplicity and speed of systemd-boot versus the complexity and bulk of GRUB2 just won me over.\nMind you, GRUB2 is very good at what it does. It’s just that I don’t need all it does. All I need is to boot my machine, nothing more, nothing less. My laptop, on which I am typing this, only has OpenSUSE installed. No dual boot, nothing. My desktop boots OpenSUSE and FreeBSD at the moment, but they do so from seperate disks with their own bootloader. Nice and simple.\nNeither need the features of GRUB2.\nThat said, changing the bootloader is tricky. It’d be nice if OpenSUSE would let you pick systemd-boot at install, so you don’t have to muck with it. However, since end of September, OpenSUSE officially supports systemd-boot.\nSo, I just had to have it, starting with my laptop.\nHOW-TO # Note: I do not use secureboot. Do not blindly follow this tutorial if you are using secure boot: it will break your system. You can do it, but it requires more steps then I will outline here.\nFirst, check if /usr/lib/bootloader/systemd-boot exists. If it does, you’re good.\nRemove all OpenSUSE entries from the EFI boot menu, and all others you do not need:\n# efibootmgr --delete --label opensuse-secureboot Do this for each entry you do not need or want. You can see what entries there are by just typinge efibootmgr without any options.\nNext we want to update the configuration, so it knows we are going to use systemd-boot.\n# vi /etc/sysconfig/bootloader Change from (probably) LOADER_TYPE=\u0026ldquo;grub2-efi\u0026rdquo; to LOADER_TYPE=\u0026ldquo;systemd-boot\u0026rdquo;\nInstall the systemd-boot utils to add support for snapshots. We are running OpenSUSE, we do want support for our automatic btrfs snapper snapshots.\n# zypper in sdbootutil-snapper sdbootutil-rpm-scriptlets If zypper complains that it needs to remove GRUB2 to do this, confirm by choosing the solution that will do so. It will both add all the files required for systemd-boot to function and remove all grub2 related files that you will not be using anymore.\nThen we want to install our kernels, including snapshots so we can actually boot as we do not have a bootmanager installed right now!\n# sdbootutil install # sdbootutil add-all-kernels Now we’re done. Systemd-boot is installed, GRUB2 is removed and we can reboot our system and enjoy a fast and less bloated setup. If you want to get to the boot menu to boot from a snapshot, just hold the spacebar while booting and it will pop up.\nUpdates # Since this article has been written, systemd-boot has been officially added to the boot options of OpenSUSE Tumbleweed and it is now available during install on an UEFI system.\n","date":"18 January 2024","externalUrl":null,"permalink":"/posts/opensuse/sdboot/","section":"Articles","summary":"Introduction # I am weird sometimes, I know.","title":"systemd-boot on OpenSUSE","type":"posts"},{"content":"Maybe it is time for a little update, even if there is not much to say. After 4 months of running OpenSUSE Tumbleweed, I can honestly say I have not had a single moment where I\u0026rsquo;d thought I should maybe look for another distribution because XYZ is getting too annoying.\nOf course it does have some annoyances. Some are with me, like running systemd-boot while it is not officially ready for primetime yet. Others are with OpenSUSE or my setup, like sometimes an update will bork the system and I have to do a snapper rollback. Fortunately, OpenSUSE makes this a 2 minute job and so far, rolling back the update and installing it again is all that is needed to fix the system and the update.\nOther than that, I can\u0026rsquo;t even really think of anything. It just works. It runs, it runs fast, I have all the latest packages as you can expect using a rolling release and I just get to enjoy using my computer and play around with it. Both my desktop as well as my laptop.\nIt\u0026rsquo;s nice to be able to use the computers without this itch or little doubt that makes one go and look for greener pastures.\nI\u0026rsquo;m still impressed, really impressed with OpenSUSE. More should be using it, it really is that good. And a bit of counterweight to the giants like Canonical and Redhat would be nice too.\nHave a good one.\n","date":"4 December 2023","externalUrl":null,"permalink":"/posts/opensuse/4months/","section":"Articles","summary":"Maybe it is time for a little update, even if there is not much to say.","title":"OpenSUSE: 4 months in","type":"posts"},{"content":"","date":"13 October 2023","externalUrl":null,"permalink":"/tags/hyprland/","section":"Tags","summary":"","title":"Hyprland","type":"tags"},{"content":"The other night, I was playing around watching YouTube videos and was watching this one video from Brodie Robertson on how he was now using Hyprland as his daily driver. Usually, I just zone out to other tasks when the topic gets to a tiling window manager, as it is not for me: I have a ultrawide screen on my desktop, with a 3440x1440 resolution and trust me, you do not want to run console fullscreen.\nBut, as I was sitting with my laptop in my lap which obviously has a more common resolution display, I thought what the heck. Let\u0026rsquo;s see if OpenSUSE offers Hyprland in its repos and see what it\u0026rsquo;s all about.\nWell, it was in the repos and to my suprise it didn\u0026rsquo;t suck. I mean, I still prefer a normal desktop for desktop tasks, but for managing my network and homelab from my laptop, where I pretty much live in (multiple) terminal sessions and a some of browser tabs on a different workspace, this works. And pretty good too, with great focus on what you need to achieve.\nI like, again to my suprise\nOn a sidenote: does anyone know how to use tiling window manager on a ultrawide screen, where you do not want a single application to go fullscreen, but remain restricted to just part of the screen, let\u0026rsquo;s say a third of the screen. Or top/bottom half vertically and a third horizontally. Having essentially two screens side by side in a single panel is a dream from a screen real estate point of view, but its a nightmare for using a tiling window manager imo, especially if you take care organizing your windows.\n","date":"13 October 2023","externalUrl":null,"permalink":"/posts/linux/interresting/","section":"Articles","summary":"The other night, I was playing around watching YouTube videos and was watching this one video from Brodie Robertson on how he was now using Hyprland as his daily driver.","title":"Interesting (or: first tiling windows manager experience)","type":"posts"},{"content":"","date":"10 October 2023","externalUrl":null,"permalink":"/categories/bsd/","section":"Categories","summary":"","title":"BSD","type":"categories"},{"content":"","date":"10 October 2023","externalUrl":null,"permalink":"/tags/freebsd/","section":"Tags","summary":"","title":"Freebsd","type":"tags"},{"content":"No, I did not stop running OpenSUSE Tumbleweed. That\u0026rsquo;s still my daily driver. I did however wipe my Windows drive and installed FreeBSD instead. I have used FreeBSD quite a lot way back when, and since I hadn\u0026rsquo;t actually booted into Windows for months, I just decided to install FreeBSD on that drive so I could play with it again after I saw KDE is actively supporting FreeBSD for the desktop.\nInstalling FreeBSD is actually pretty straight forward. It uses an ncurses installer that was common on BSD and Linux alike 25 years ago, but today it\u0026rsquo;s basically only seen on FreeBSD and Slackware Linux. You can however use that GUI over SSH, so pretty convenient for server installs.\nAfter answering a few simple questions like locale, disk and services you\u0026rsquo;re essentially done and prompted to reboot. Then comes the scare for the modern Linux user: FreeBSD boots into a command prompt. No GUI. Not installed.\nOops. 🙂\nSeriously, not that complicated either. It\u0026rsquo;s just that you have consciously decide you want it, and what you want. That\u0026rsquo;s not a bad thing. Arch Linux users will understand.\nTo use KDE on FreeBSD, we need to install some things and configure a few others. To install:\npkg install --quiet --yes kde5 plasma5-sddm-kcm sddm xorg If you are running an nvidia graphics card, do yourself a favor and also install:\npkg install --quiet --yes nvidia-xconfig Then run the following commands to configure (as root):\nnvidia-xconfig sysrc dbus_enable=\u0026quot;YES\u0026quot; \u0026amp;\u0026amp; service dbus start sysrc sddm_enable=\u0026quot;YES\u0026quot; \u0026amp;\u0026amp; service sddm start The SDDM login screen should start and you should be able to login to KDE. Do make sure you choose X11 for session, not Wayland as Wayland is extremely buggy on FreeBSD at best.\nIf no GUI, try to reboot first and if that doesn\u0026rsquo;t work, check if you have an xorg.conf under /usr/local/etc/X11 as this should have been generated by the nvidia-xconfig utility, but without xorg config nothing will happen.\n","date":"10 October 2023","externalUrl":null,"permalink":"/posts/2023-10-10-no-more-windows-freebsd-instead/","section":"Articles","summary":"No, I did not stop running OpenSUSE Tumbleweed.","title":"No more Windows. FreeBSD instead.","type":"posts"},{"content":" Introduction # Since I have moved to running Linux basically full time, I have ran Pop!OS, Fedora, Nobara, back to Fedora KDE Spin and now OpenSUSE Tumbleweed. All of them are good distributions and none have any real deal-breakers for me not to want to use them.\nThe fact that I really, really like Tumbleweed was not a given. Quite the contrary. OpenSUSE does it\u0026rsquo;s own thing, so it is a bit of a learning curve if you are used to Debian-based or RedHat-based distributions. It is one of the oldest distributions still operating today, which puts it in the same company as Debian and Slackware. The latter actually being the initial origins for (then) SUSE Linux.\nBut I digress. My experience with SUSE in the past (=20 odd years ago) was less than favorable to the point that I abandoned it and never really looked at it again until now. Installation was painless and it let you choose your desired desktop environment as part of the installation procedure. I run KDE, which is the first option and let it do its thing.\nAfter install I was back in business. Mind you: I have /home on a seperate disk, so all I have to do when reinstalling or using a new distribution is to make sure that drive is mounted on /home and I have all my (program) settings back and my desktop just looks the same as before. Saves me time to set everything up the way I want again and no data loss when doing a reinstall.\nObervations # Some observations after installing OpenSUSE Tumbleweed:\nUsing OpenSUSE means using BTRFS right: proper subvolumes upon install, snapper configured out of the box to create snapshots when installing software and updates and grub configured to be able to boot from previously created snapshots so you can rollback if something happens to go wrong. No more worried about effing up your system YaST2 is a very useful tool. It let me configure and add my iscsi targets in 2 minutes tops and it connects automatically on boot again, whereas most other distributions make that a painful, manual task that requires workarounds to mount your drives after a reboot. I don\u0026rsquo;t really need a GUI tool to configure my systems, but this tool lets you configure all aspects of the system and sometimes it is just faster doing it once in the GUI than to do it manually. Note: the original YaST is the reason I quite SUSE years ago and never came back. Different story. It has a huge repository of programs, including non-FOSS if you add the packman repos. And if you install opi (openSUSE Package Installer) you have the entire OBS (OpenBuild System) at your finger tips, including user repositories. Basically you have the OpenSUSE equivalent of Arch\u0026rsquo;s AUR. Installation of proprietary drivers and codecs is easy and painless, as with most distributions. That said, I think OpenSUSE is the only distro where the Nvidia drivers come straight from a repo at Nvidia. It\u0026rsquo;s (Tumbleweed) a rolling distribution, so always the latest software but it is better tested than something like Arch. This means less issues are likely to arise. Inconveniences # Both my laptop and desktop were unable to load Wayland and had to use Xorg. This is not a big thing. This was caused by a SDDM bug where the SHELL variable got messed up if you use the fish shell as you login shell. This has been fixed by the KDE team in \u0026lsquo;21/\u0026lsquo;22, but the fix was never backported to OpenSUSE by the OpenSUSE team. Workaround was to change the login shell to standard bash, and just tell the terminal app to start with the fish shell. Now I can run Wayland on my laptop, while still using Xorg on my desktop as it work just works better with Nvidia graphics. But it will run Wayland if I want to. Other stuff # Other things I did after installing Tumbleweed are not OpenSUSE specific things, but more generic Linux optimizations I have made.\nI used tuned with tuned-adm to select and set an appropriate tuning profile for my systems. I have set up a couple scripts for my laptop to switch profiles and CPU modes depending on if the laptop is running on AC power, or battery. When running on AC power:\n#!/bin/bash # # Set CPU governor to powersave if [ -e /usr/bin/cpupower ]; then sudo cpupower frequency-set --governor performance sudo tuned-adm profile latency-performance else echo performance | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor sudo tuned-adm profile latency-performance fi # EOF When running on battery power:\n#!/bin/bash # # Set CPU governor to powersave if \u0026amp;#91; -e /usr/bin/cpupower ]; then sudo cpupower frequency-set --governor powersave sudo tuned-adm profile powersave else echo powersave | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor sudo tuned-adm profile powersave fi # EOF These are set in KDE settings under power management, see screenshot: On the desktop, I just set the tuned profile to latency-performance using tuned-adm. This is a profile that aims to prioritize latency over throughput, which is great for desktop use with a GUI. I also set some sysctl parameters as outlined in another post. Like I said, these are not Tumbleweed or OpenSUSE specific optimizations or even necessary, but it is how I prefer to set up my systems. I feel it helps me to squeeze out the most performance out of my systems, and in the case of the laptop, preserve battery life when needed.\nConclusion # When all is said and done, I really like OpenSUSE Tumbleweed. I like the bootable snapshots. I like having the latest versions of different software without having to wait for a next release. I like the tools provided to admin the system. It\u0026rsquo;s a good\u0026rsquo;un. I think I\u0026rsquo;ll keep it. Then again, I said that of Fedora. And Nobara. But I do think it is true this time.\n","date":"4 August 2023","externalUrl":null,"permalink":"/the-end-to-distro-hopping/","section":"Articles","summary":"Introduction # Since I have moved to running Linux basically full time, I have ran Pop!","title":"The end to distro-hopping?","type":"post"},{"content":"","date":"3 May 2023","externalUrl":null,"permalink":"/tags/containers/","section":"Tags","summary":"","title":"Containers","type":"tags"},{"content":"","date":"3 May 2023","externalUrl":null,"permalink":"/categories/containers/","section":"Categories","summary":"","title":"Containers","type":"categories"},{"content":"","date":"3 May 2023","externalUrl":null,"permalink":"/tags/homelab/","section":"Tags","summary":"","title":"Homelab","type":"tags"},{"content":"","date":"3 May 2023","externalUrl":null,"permalink":"/categories/homelab/","section":"Categories","summary":"","title":"Homelab","type":"categories"},{"content":" Introduction # This is a quick post on how to run multiple instances of AdGuard@Home on a single Synology host using Docker.\nProblem: you cannot easily, using Docker, run multiple instances of the same program - or different program - while listening on the same port.\nSolution: do not use host or bridge networking, but put the container on the same network as the host using macvlan.\nTo achieve this, we need to do the following:\nPrerequisites # Find the name of the network interface your Synology is using to connect to the network you want your Docker containers to be running on. This can be for instance eth0 for a single interface, or bond0 for when you use channel bonding. You can find this under Control Panel \u0026gt; Network \u0026gt; Network interface. In my case, this is bond0 which is what I will use in the examples below. Configuring the interface # Now we have to configure the interface Docker can use. We do this by adding a bridge on top of the existing physical interface you use on your network.\nip link add macvlan-br0 link bond0 type macvlan mode bridge\nThis adds a bridge device on top of the physical interface with the name macvlan-br0 ip addr add 192.168.0.254/32 dev macvlan-br0\nThis adds an IP address on the bridge device so the host has an IP address in the range will give to Docker ip link set macvlan-br0 up\nThis will activate the virtual bridge device ip route add 192.168.0.192/26 dev macvlan-br0\nThis will add a route to the Docker network so it can be reached You will have to put this in a script you can run at boot of your Synology device, as these settings will not retain over a reboot as we have to make them on the commandline and cannot make them in the Synology DSM.\n#!/bin/bash # # Set timeout to wait host network is up and running sleep 60 # # Recreate the host macvlan bridge ip link add macvlan-br0 link bond0 type macvlan mode bridge ip addr add 192.168.0.254/32 dev macvlan-br0 ip link set macvlan-br0 up ip route add 192.168.0.192/26 dev macvlan-br0\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt; Docker # Now that we have set up the host, we can continue creating a new network in Docker that can be used by our containers. To do this, type:\ndocker network create -d macvlan \\ --subnet=192.168.0.0/24 \\ --gateway=192.168.0.1 \\ --ip-range=192.168.0.192/26 \\ --aux-address 'host=192.168.0.50' \\ -o parent=bond0 macvlan-br0 This will create a network in Docker on the subnet of your network in a dedicated range of IP addresses using your physical interface and virtual bridge device.\nDo make sure the IP range you specify for Docker is not part of your DHCP scope, if you are also running DHCP or you will get IP conflicts. Docker does not use DHCP, and instead will just hand out IP addresses from this range in order to each container.\nNow you can create your Docker containers as usual and configure them to use this network, instead of the standard host or bridge networks Docker uses by default. You can also assign this network to existing containers if you want.\nInstall container # To install AdGuard@Home on your Synology, open Docker, go to Registry and search for adguard\nDouble click to download the latest version.\nWhen it is done downloading, you can go to the Container section, and hit create. A window will popup where you can select your freshly downloaded image. Select it and click next to follow the instructions. Configure what you want, but make sure to select the macvlan network when you get to the screen to pick the network.\nCongratulations! Test your new container that is now present on your local network.\nImportant note: if you are running the DNS server on your Synology for your local network, and you want to keep using that, make sure you configure your AdGuard as DNS for your clients, and add the following to your AdGuard DNS configuration under upstream servers:\n[/domain.local/]192.168.0.254:53 This will instruct AdGuard to use the IP address you added to your bridge device at the beginning as the source for resolving domain.local hosts.\nThe complete upstream section for me looks something like (I use Cloudfare DNS for internet, and Synology DNS for local addresses):\ntls://1.1.1.1 tls://1.0.0.1 [/domain.local/]192.168.0.254:53 Do not use the real IP address of your Synology host; this is not reachable for Docker and will not work! Use the bridge device address instead.\n","date":"3 May 2023","externalUrl":null,"permalink":"/posts/linux/adguard/","section":"Articles","summary":"Introduction # This is a quick post on how to run multiple instances of AdGuard@Home on a single Synology host using Docker.","title":"Running redundant AdGuard@Home DNS servers on Synology","type":"posts"},{"content":"","date":"8 March 2023","externalUrl":null,"permalink":"/tags/f1/","section":"Tags","summary":"","title":"F1","type":"tags"},{"content":"","date":"8 March 2023","externalUrl":null,"permalink":"/tags/f1academy/","section":"Tags","summary":"","title":"F1Academy","type":"tags"},{"content":"","date":"8 March 2023","externalUrl":null,"permalink":"/tags/racing/","section":"Tags","summary":"","title":"Racing","type":"tags"},{"content":" Introduction # Before the start of the 2023 F1 season, Formula 1 launched the F1 Academy initiative to be headed by Susie Wolf and get more girls into F1. It is a girl-only series racing on the same tracks F1 is racing. While this is a noble cause, it is my opinion this is the wrong thing to do and it is to the detriment of all girls and women in racing. It will show women as less than men in racing and ultimately achieve the exact opposite of what it wants to do.\nNumbers # Let me explain.\nThere are far less girls than boys that want to get into racing at an early age. Some of the reasons are just the difference in interests between boys and girls, others have more to do with culture and how boys and girls are raised differently by their parents. Some of these things can (and arguably should) be addressed. But they are not by the F1 Academy.\nSo lets assume for arguments sake that for every 10,000 boys wanting to get into racing each year, about 10% of that number is girls wanting the same thing. That’s probably not far off.\nLet’s also assume that of all those boys and girls 1% has the drive to do whatever it takes to achieve their dreams in racing from 5-6 years old starting in karting, all the way through the classes until they are 18 and could be picked up by an F1 team.\nThat’s 100 boys vs 10 girls that have the drive to do what it takes. And this still does not take into account ability.\nIf you have maybe 1 or 2 seats available each year for someone that is perceived to be as good to potentially be the next Max, Lewis, Charles or Fernando and to be picked up by an F1 team, and assuming all of those fought hard enough to end up in the open wheel feeder series by age 18, the chance of that someone to be a guy is near 100%. And the chance of that someone being a gal is not zero, but that’s about it.\nIt’s just a numbers game.\nDownsides # Setting up a separate girls-only series together with F1 is not going to increase those numbers. What it does is pit girls against other girls that are not good enough to compete with their male peers for the same seat. Girls that have difficulty making the top 10 in F4. They’re not good enough for F3, let alone F1. And they are going to fail and burn out once they are pitted against the big boys in F1 because they are selected solely on their gender instead of their quality as a racing driver, assuming they would get a seat which is unlikely. Regardless, the perception is going to be that girls suck at racing compared to boys, which they don’t. But that will turn off girls from racing again, because they will feel they are not good enough anyway. They’ll give up before even trying. Which means less girls competing, instead of more.\nF1 does not want boys. It wants talent. But having a girl would be marketing gold. So if the teams could pick up a girl that was at say 75% capability of a Lewis Hamilton or Max Verstappen or Charles LeClerc, they would be all over her like flies on poo and fighting to sign her. The fact that this isn’t happening, isn’t because F1 teams favor boys, but simply there not being enough quality left in the pool of hopefuls. There’s only 22 racing seats in Formula 1. Out of a global pool of racing drivers, you need to really stand out to even have a chance, regardless of gender.\nTo be in F1, you need to be good, really good, even if you are a pay-driver. The ones that get in and suck are still head and shoulders above the others, as is often demonstrated by their results in other classes after they are dumped from F1. If a no good pay-driver washes out of F1 because they are not up to snuff can continue their career in Indycar, Formula E, World Endurance Championship, etc and win (even championships), they are not lacking talent. They are really, really good drivers. They were just weighed in the balance of F1 and found wanting.\nThe only way to get more girls to have a shot of getting into F1 is to get more girls interested in racing at an early age. To get close to 50% participation to even the odds. To fight the stereotypes of boys = cars, girls = dolls when raising our children. To not only tell them they are equal, but raise them to be equal. To perhaps financially support girls that are showing to be good enough to grow into the role models needed to sustain the system on its own if they would otherwise be forced to drop out. But you can’t start in your teens and expect to catch up and compete with boys that have been racing their entire life. It just doesn’t happen.\nBut still, if there are not enough girls that want to race, you are not going to have enough quality persisting all the way to F1. Maybe we need to also accept that boys and girls are equal but not the same. That they have different interests and motivations. That, on average, boys are physically stronger than girls. That, on average, boys have more competitive drive than girls. That it’s ok to be different. And accept the numbers are stacked against girls because of it.\nConclusion # Equal opportunity does not necessarily result in equal representation. And that’s ok, as long the result is not skewed by discrimination of any sort.\nBut setting up yet another women-only class because there aren’t any women in F1 is not going to pull women up in racing. It will put them down.\n","date":"8 March 2023","externalUrl":null,"permalink":"/posts/stuff/2023-03-08-why-the-f1-academy-is-bad-for-women-in-racing/","section":"Articles","summary":"Introduction # Before the start of the 2023 F1 season, Formula 1 launched the F1 Academy initiative to be headed by Susie Wolf and get more girls into F1.","title":"Why the F1 Academy is bad for women in racing","type":"posts"},{"content":" Introduction # I finally got fed up with my Macbook Pro as a Linux laptop. It was a 2012 model, no longer supported by Apple. But the quirky implementation of Nvidia discrete graphics on the Macbook wasn\u0026rsquo;t the best for reliable Linux operation. Mind you, Nvidia is always a bit of a challenge, but Apple being Apple certainly does not help in that regard.\nLaptop # Anyway, I am currently sporting a Lenovo T460 from 2016. Still not really a new laptop, but fast enough for Linux and most things I do with it. It has a i5 6300U from the Skylake family of processors, 8GB memory (for now, supports up to 32GB) and a 1TB SSD. For graphics it used the Intel 520\nOperating System # For the OS, I am now running Nobara Linux 37. This is a Fedora derivative with tweaks for performance and gaming. While I am not a gamer, the improvements do help making it a snappy experience. Contrary to my desktop computer, I am now using the KDE version.\nDesktop environment # Getting used to KDE after having used Gnome for a long time can cause some frustrations. Not because what you want isn\u0026rsquo;t possible (typically quite the opposite) but because you cannot find it or know where to look. Where Gnome is has more of a our way or the highway\u0026rsquo; mentality and is more concerned with their vision than how the users are perceiving or using it, KDE is all about customization and providing the user with the ultimate choice. If you can dream it, KDE can probably be configured to do it. And that can be a bit overwhelming. It has so many options, sometimes you just have no idea where to look to achieve something.\nConclusion # So far, I am very happy. Both with the laptop, as well as Nobara Linux and KDE. I may have to switch my desktop to the same configuration soon. Now that I am getting the hang of KDE again, I remember why it was my default and go to desktop environment in the past, and the quirks and annoyances from Gnome seem to be more and more stuff I don\u0026rsquo;t really want to deal with no more. You shouldn\u0026rsquo;t have to hack together your environment around the limitations imposed by its maintainers. It\u0026rsquo;s Linux, not Windows or MacOS.\n","date":"20 January 2023","externalUrl":null,"permalink":"/posts/linux/new-laptop/","section":"Articles","summary":"Introduction # I finally got fed up with my Macbook Pro as a Linux laptop.","title":"New laptop, new Linux distro","type":"posts"},{"content":"","date":"15 May 2022","externalUrl":null,"permalink":"/tags/fedora/","section":"Tags","summary":"","title":"Fedora","type":"tags"},{"content":" Upgrade was smooth as is to be expected with Fedora and everything is running smooth. Libadwaita is not as bad as I thought it to be, and it actually looks really good. I wish they had taken the time and effort to create a GTK3 theme for applications that are not libadwaita aware, but fortunately someone else did. It does look horribly inconsistent without. Bad move.\nOther than this, I do not miss the theming at the moment now that I added some transparency in the top bar and dock.\n","date":"15 May 2022","externalUrl":null,"permalink":"/posts/linux/fedora36/","section":"Articles","summary":"Upgrade was smooth as is to be expected with Fedora and everything is running smooth.","title":"Upgraded laptop to Fedora 36","type":"posts"},{"content":" Introduction # My daily driver is currently Pop!_OS, which is a desktop Linux distribution. It\u0026rsquo;s a very nice distribution, really good with Nvidia hardware (which isn\u0026rsquo;t a given on Linux) and, to me, a Gnome look that is very close to what I want so my GUI changes are minimal.\nWhat\u0026rsquo;s less, and that is a more generic Linux problem, is that particularly the Linux kernel is optimized for server use and not desktop. It prioritizes throughput over latency, which is great for raw performance but less if you expect a smooth, fast GUI.\nWe can fix that.\nKernel # This first one is optional and controversial. Many will say a custom kernel is not needed and does not add anything. On my computer however, using the Xanmod kernel does make the GUI significantly faster and smoother. Installation instruction are on their page.\nSecond, we want to pass two boot parameters to the kernel when booting. If using systemd-boot, like Pop! does, open the corresponding file under /boot/efi/loader/entries and add:\nnvme_core.default_ps_max_latency_us=0 pcie_aspm=off to the line starting with options.\nIf using grub, add the same to the line GRUB_CMDLINE_LINUX_DEFAULT under /etc/default/grub and do a update-grub to activate.\nOptions # Part two is modifying the sysctl parameters. Under /etc/sysctl.d you will find files that set certain parameters on how your system works. Create a new file, and add the following:\n# These are settings from /etc/sysctl.d/ and can be activated by running sysctl --system as root # Save this file in that location. # # These settings set the disk caching for the system # vm.dirty_bytes = 33554432 vm.dirty_background_bytes = 8388608 vm.dirty_writeback_centisecs = 100 vm.dirty_expire_centisecs = 300 # # We need to either use *_ratio, or we need to use *_bytes. We cannot use both. Currently # using _bytes, so disabling _ratio # # vm.dirty_background_ratio = 10 # vm.dirty_ratio = 80 # vm.page-cluster = 0 # Increased to improve random IO performance fs.aio-max-nr = 1048576\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt; This will set certain parameters pertaining to disk caching and IO performance. You can activate by running sysctl system as root, or by rebooting your system.\nDisk # To optimize your disks, if you are using SSD, it\u0026rsquo;s worth it to make some changes to your /etc/fstab. There\u0026rsquo;s two parts to this:\nMount the root filesystem with settings optimized for SSD\u0026rsquo;s Ensure temporary directories are running from memory by mounting them in a tmpfs to limit disk writes and extend the life of your SSD. For the first one, I mount my root device in /etc/fstab like:\ndevice / btrfs defaults,ssd,noatime 0 0 For the second, add these lines to /etc/fstab\n# SSD tweak: temporary directories as tmpfs tmpfs /tmp tmpfs defaults,noatime,mode=1777 0 0 tmpfs /var/tmp tmpfs defaults,noatime,mode=1777 0 0 tmpfs /var/log tmpfs defaults,noatime,mode=0755 0 0 tmpfs /var/spool tmpfs defaults,noatime,mode=1777 0 0 DISCLAIMER: Putting anything other than /tmp into memory, can produce unpredictable results in specific circumstances. It should be ok on desktop machines and helps to extend the life of your SSD by limiting writes. Do not enable on servers. Actually most of what is on this page may have an adverse effect on server performance.\nActivate by rebooting your system. Enjoy a faster, more responsive system.\n[FEB 8/22 UPDATE]: Since publishing this article I have moved from Pop!_OS to Fedora. Fedora is a cutting edge distribution, which does not require all of these tune-ups to make a snappy OS out of the box.\nLet\u0026rsquo;s elaborate.\nThe kernel parameters mentioned above do not need to be updated on Fedora The sysctl.d modifications are not required on Fedora, but they are done simply because I have more than plenty memory anyway. Out of the box default settings on Fedora are better than those of Pop!_OS The disk optimizations in /etc/fstab are set by default Fedora Update # Fedora uses a different package management system than Pop! OS, which is Ubuntu based. While Debian derivatives like Ubuntu and Pop!_OS use apt as their package manager, Fedora is RedHat based. RedHat uses rpm files which are managed by yum or dnf (depending on the version of the OS).\nBy default, dnf is quite slow compared to apt but this is easily fixed by adding some parameters to the configuration file.\n[main] gpgcheck=1 installonly_limit=3 clean_requirements_on_remove=True best=False skip_if_unavailable=True \u0026amp;lt;strong\u0026gt;max_parallel_downloads=10 fastestmirror=True The two bottom bold lines need to be added to /etc/dnf/dnf.conf. The first one increases the number of simultaneous download connections to 20, which increases download speed. The second one looks for the fastest mirror from your location, which ensures you will get the maximum possible download speed. Combined, these make dnf operate as fast or faster than apt on most systems.\nDue to the nature of a bleeding edge distribution like Fedora, it can sometimes be tricky to update. Especially the kernel and / or kernel drivers. To avoid such problems, I run updates with the --exclude=kernel* flag. In fact, I wrote a function for my Fishshell to get and install updates without kernel, like so:\nfunction up2date sudo dnf upgrade -y --refresh --exclude=kernel\\* end And saved it as up2date.fish under $HOME/.config/fish/functions\n","date":"17 November 2021","externalUrl":null,"permalink":"/posts/linux/performance/","section":"Articles","summary":"Introduction # My daily driver is currently Pop!","title":"Optimizing Linux for desktop performance","type":"posts"},{"content":"","date":"17 November 2021","externalUrl":null,"permalink":"/tags/performance/","section":"Tags","summary":"","title":"Performance","type":"tags"},{"content":" Introduction # Recently I had to move my Linux install from one drive to another, as I was experiencing some issues with a WD SN550 nvme drive causing some short random freezes of the GUI with IO intensive tasks. Since I also have a Samsung nvme drive installed, I decided to see if the problem persists on the other drive.\nWhat now # But. Having a fully configured and customized Linux install is a pain in the behind to redo. I did not want to clone, because I made a mistake during install the previous time and it installed in legacy MBR mode, so I wanted to do a proper install using UEFI mode. But preferably not having to re-do all the setup and customizations.\nAnd I didn\u0026rsquo;t have to. Apt to the rescue.\napt-mark showauto pkgs_auto.lst apt-mark showmanual pkgs_manual.lst This will generate a list of .deb packages installed on the system. The first one with all the automatically installed packages, the second one all the manually installed packages from the commandline.\nI also made a backup of /etc/apt/sources.list.d and /etc/apt/trusted.gpg.d. The first directory contains all the repositories I had in use on the original install, and the second directory holds all the GPG keys that go with these repositories. Important!\nFirst install the system on the new drive, and make sure all updates are installed. You don\u0026rsquo;t need to install or setup anything but the base system. Now you can continue with the back ups and files you created earlier.\nAfter I moved the two directories above to their respective place on the new install, and of course doing an sudo apt update \u0026amp;\u0026amp; sudo apt upgrade to make sure all packages are still up to date, I loaded up the list of packages I created earlier.\nsudo apt install $(cat pkgs_auto.lst) sudo apt install $(cat pkgs_manual.lst) This will create some errors due to packages that cannot be installed like this, or packages that were installed from a .deb file and aren\u0026rsquo;t located in any repository. Clean up those entries, and try again and let it run.\nWhen it is finished, copy your /home/ from your old drive to your new drive and when you reboot and log back in as your user, everything should be just as it was before.\nSuccess!\n","date":"8 November 2021","externalUrl":null,"permalink":"/posts/linux/moving/","section":"Articles","summary":"Introduction # Recently I had to move my Linux install from one drive to another, as I was experiencing some issues with a WD SN550 nvme drive causing some short random freezes of the GUI with IO intensive tasks.","title":"Moving a Linux install to a new disk","type":"posts"},{"content":"","date":"26 July 2021","externalUrl":null,"permalink":"/tags/grafana/","section":"Tags","summary":"","title":"Grafana","type":"tags"},{"content":"","date":"26 July 2021","externalUrl":null,"permalink":"/categories/monitoring/","section":"Categories","summary":"","title":"Monitoring","type":"categories"},{"content":" Introduction # For the longest time, I have been monitoring my network and homelab using Observium. This worked and does work very well. Observium is very good at what it does. However, there are a couple of things that do not work so well for me using Observium:\nObservium does not let me add applications to monitor very easily or at all Observium is limited to what can be offered through SNMP Observium is not open source and as such it cannot be modified or changed to your needs Observium is not an application I have come across in my professional life, so knowing how it works does not help me professionally. That last bit is obviously not a necessity, however I do feel it\u0026rsquo;s always a nice thing to be able to apply things you have learned in your homelab into your professional environment.\nZabbix # After lots of investigations and trial \u0026amp; error, I have settled on using Zabbix for my monitoring needs. Zabbix is open source product that is used a lot in corporate environments and it is very flexible and extensible. Obviously you could add or change code as it is open source, but you don\u0026rsquo;t really need to. As Zabbix is template driven, its functionality can be extended by adding templates and there is a plethora of templates available for Zabbix, both on the Zabbix site itself (Zabbix Share) as well as places like GitHub. Also, Zabbix can use an agent installed on the system to collect the data you want to monitor, or you can use SNMP if you can\u0026rsquo;t or don\u0026rsquo;t want to install an agent on a device (for instance a network router).\nThe one thing I do not like about Zabbix is that the historic view is not easy to get to, nor as pretty displayed in a dashboard-like view as it is in Observium. However, that is not a blocker as we can use Grafana, another open source tool that is used quite a bit in Corporate Land to create dashboards and display relevant historic data.\nInstalling Zabbix # Installing Zabbix is no more complicated than installing most other software on Linux. I installed Zabbix on my Raspberry Pi 2B and the short version is this:\na. Install the Zabbix repository:\n# wget \u0026lt;https://repo.zabbix.com/zabbix/5.4/raspbian/pool/main/z/zabbix-release/zabbix-release_5.4-1+debian10_all.deb\u0026gt; # dpkg -i zabbix-release_5.4-1+debian10_all.deb # apt update b. Install the Zabbix server, frontend and agent on the server machine\n# apt install zabbix-server-mysql zabbix-frontend-php zabbix-apache-conf zabbix-sql-scripts zabbix-agent c. Create the database for Zabbix\n# mysql -uroot -ppassword mysql\u0026gt; create database zabbix character set utf8 collate utf8_bin; mysql\u0026gt; create user zabbix@localhost identified by 'password'; mysql\u0026gt; grant all privileges on zabbix.* to zabbix@localhost; mysql\u0026gt; quit; And import the database schema\n# zcat /usr/share/doc/zabbix-sql-scripts/mysql/create.sql.gz | mysql -uzabbix -p zabbix If your database is not residing on your Zabbix server, you can add -h to the above to make sure you are connecting to the remote database server. Also, when creating the user and you are using a remote database, make sure the Zabbix user is allowed to connect over the network to the database.\nd. Configure the Zabbix server to use the database you have created by filling out the relevant fields in /etc/zabbix/zabbix_server.conf\ne. Start your brand-new Zabbix server:\n# systemctl restart zabbix-server zabbix-agent apache2 # systemctl enable zabbix-server zabbix-agent apache2 f. You are done. You can now login to your server on http://:3000 and log in with user Admin and password zabbix and configure everything else using the web frontend.\nInstalling Grafana # Installing Grafana is equally simple.\na. Install the pre-requisites and add the repository key\nsudo apt-get install -y apt-transport-https sudo apt-get install -y software-properties-common wget wget -q -O - \u0026lt;https://packages.grafana.com/gpg.key\u0026gt; | sudo apt-key add -` b. Add the repository\necho \u0026quot;deb \u0026lt;https://packages.grafana.com/oss/deb\u0026gt; stable main\u0026quot; | sudo tee -a /etc/apt/sources.list.d/grafana.list c. Install Grafana\napt update\u0026lt;br\u0026gt;apt install grafana d. Then it is just a matter of starting the Grafana server and making sure it starts at boot.\nsudo systemctl daemon-reload sudo systemctl start grafana-server sudo systemctl status grafana-server sudo systemctl enable grafan-server That\u0026rsquo;s it! You can now log in using the default username/password of admin/admin.\nNote: if you want to apply specific configurations, like for instance database (mysql, postgres, sqlite3) beyond the default, you should refer the Grafana manuals as that would be a bit beyond the scope of this page.\nTying things together # First you will need to install the Zabbix app into Grafana. We can do this using the commandline interface for Grafana:\ngrafana-cli plugins install alexanderzobnin-zabbix-app After you do this, you can configure the Zabbix datasource. Go to Configuration -\u0026gt; Data sources and click add data source. Scroll down to the bottom, and you will see one called Zabbix'.\nFill in the details of your Zabbix installation, and make sure you add the api_jsonrpc.php to the end of your URL. Check With credentials\u0026rsquo; under auth and under Zabbix API details add your username and password. Click Save \u0026amp; test and if all is ok, it will give you a green checkmark while saying data source updated.\nYou are now ready to add a dashboard to your Grafana and start monitoring your Zabbix data. I use this dashboard from Paulo Paim. You can add it by going to Dashboards -\u0026gt; Manage and then clicking the import button. In the box saying import from grafana.com, type the ID of the dashboard. In this case 5363 and click load.\nThat\u0026rsquo;s it!\nLinks # Zabbix manual Grafana documentation Zabbix templates and add-ons Grafana dashboards and plugins ","date":"26 July 2021","externalUrl":null,"permalink":"/monitoring-your-network-or-homelab-using-zabbix-and-grafana/","section":"Articles","summary":"Introduction # For the longest time, I have been monitoring my network and homelab using Observium.","title":"Monitoring your network or homelab using Zabbix and Grafana","type":"post"},{"content":"","date":"26 July 2021","externalUrl":null,"permalink":"/tags/zabbix/","section":"Tags","summary":"","title":"Zabbix","type":"tags"},{"content":" Introduction # Sometimes, there is a bit of confusion as to what the difference is between application containers (Docker, Podman, K8S, OpenShift), system containers (LXC/LXD) and virtualization (KVM, Vmware, Hyper-V, Xen) and when you should use them. I will try to explain the differences as short as possible.\nApplication containers # Application containers are created with the most minimal environment to run a specific application. This includes the OS and all dependencies for that application. Every tool or program normally present in the OS that is not needed to run the application is typically left out so that the container image is a small as possible and performs as fast as possible.\nExamples of application containers are Docker, Podman, Kubernetes and OpenShift.\nSystem containers # System containers are typically as the minimal environment needed to run a specific operating system. All the basic tools are present, including the package manager so you can set up the system as you want with all the tools you need. A system container is like a virtual machine, but without the hardware virtualization layer. A system container uses and identifies the host hardware and runs the same kernel as the host. This means it is much lighter on resources than full blown virtualization, but also that in certain cases it can be incompatible with certain software.\nExamples of system containers are LXC, LXD\nVirtualization # Virtualization is software that emulates the hardware so that more than one virtual machines can be installed on the same physical hardware at the same time. As the full hardware layer needs to be simulated, it has the most overhead of these options but it also is the most compatible with all software.\nThere are two types of virtualization:\nType 1 hypervisors: virtualization is done by the kernel, providing low-level access to the physical hardware for the virtualization software for increased performance.\nExamples of Type 1 hypervisors: KVM, Vmware ESXi, Hyper-V Server Type 2 hypervisors: virtualization is done by an application installed on an operating system. Access to hardware must be done through the OS and no direct access is possible. IT provides convenience over performance, as it can run on any sytem.\nExamples of Type 2 hypervisors: Vmware Workstation, VirtualBox, Hyper-V manager ","date":"13 February 2021","externalUrl":null,"permalink":"/posts/quicky/application-containers-system-containers-virtualization/","section":"Articles","summary":"Introduction # Sometimes, there is a bit of confusion as to what the difference is between application containers (Docker, Podman, K8S, OpenShift), system containers (LXC/LXD) and virtualization (KVM, Vmware, Hyper-V, Xen) and when you should use them.","title":"As short as possible: application containers, system containers, virtualization","type":"posts"},{"content":"","date":"13 February 2021","externalUrl":null,"permalink":"/tags/kvm/","section":"Tags","summary":"","title":"Kvm","type":"tags"},{"content":"","date":"13 February 2021","externalUrl":null,"permalink":"/tags/virtualization/","section":"Tags","summary":"","title":"Virtualization","type":"tags"},{"content":"","date":"13 February 2021","externalUrl":null,"permalink":"/categories/virtualization/","section":"Categories","summary":"","title":"Virtualization","type":"categories"},{"content":" There are no RedHat Enterprise Linux (RHEL) LXC/LXD container images publicly available? There are LXC/LXD container images available for CentOS and Fedora? You can convert a CentOS install to RHEL using the Convert2RHEL tool? This also works for a LXC/LXD container? You will need to do this if you want to run RHEL on Proxmox in a LXC container? You can create a tarbal of a running system to import into a vanilla LXC/LXD installation? You will need to create a metafile.tar.gz with a few lines of information about the tarbal to do this? You can also use an export from a Docker container to get the system? And that you can also use an export from a WSL distribution for this? This means you can set up a WSL environment on your Windows box (see other posts here) just the way you want with all the tools you need, and you can export it to run it independently in a LXC/LXD container as a server? ","date":"13 February 2021","externalUrl":null,"permalink":"/posts/linux/rhel-on-lxc/","section":"Articles","summary":"There are no RedHat Enterprise Linux (RHEL) LXC/LXD container images publicly available?","title":"Did you know... (or, RHEL on LXC/LXD)","type":"posts"},{"content":"","date":"13 February 2021","externalUrl":null,"permalink":"/tags/lxc/","section":"Tags","summary":"","title":"Lxc","type":"tags"},{"content":"","date":"13 February 2021","externalUrl":null,"permalink":"/tags/lxd/","section":"Tags","summary":"","title":"Lxd","type":"tags"},{"content":"","date":"13 February 2021","externalUrl":null,"permalink":"/tags/redhat/","section":"Tags","summary":"","title":"Redhat","type":"tags"},{"content":"","date":"13 February 2021","externalUrl":null,"permalink":"/tags/rhel/","section":"Tags","summary":"","title":"Rhel","type":"tags"},{"content":"","date":"13 February 2021","externalUrl":null,"permalink":"/tags/wsl/","section":"Tags","summary":"","title":"Wsl","type":"tags"},{"content":"","date":"13 February 2021","externalUrl":null,"permalink":"/tags/wsl2/","section":"Tags","summary":"","title":"Wsl2","type":"tags"},{"content":"The following distributions I have installed/created, tested and used under WSL2 and Windows 10.\nAlpine Linux; extremely tiny; created from Docker image AlmaLinux; migrated from CentOS using AlmaLinux migration tool Arch Linux; created from virtual machine install CentOS 7; created from rootfs image, then upgraded to latest version CentOS 8; Upgraded from CentOS7 CentOS Stream; upgraded from from CentOS8 install Deepin Linux; created from virtual machine install Debian 10 (Buster); Microsoft Store Debian Testing (Bullseye); upgraded from official Debian 10 release Devuan; migrated from Debian as installed from the Microsoft Store Fedora; created from virtual machine installation Gentoo; compiled from source using Stage3 tarball/rootfs iamge Kali Linux; Microsoft Store Oracle Linux; migrated from CentOS instance using CentOS migration tool from Oracle RedHat Enterprise Linux; created from virtual machine installation Rocky Linux; migrated from CentOS using RockyLinux migration tool Ubuntu; Microsoft Store Slackware; created from Docker image As you can see, if you can find your favorite distribution in the Microsoft Store, great, if not or if its only available as a paid distribution, just bake your own. It\u0026rsquo;s not that difficult and really there is no reason why you should not have the Linux distribution of your choice available under WSL2.\n","date":"30 January 2021","externalUrl":null,"permalink":"/posts/wsl/tested-distributions-under-wsl2/","section":"Articles","summary":"The following distributions I have installed/created, tested and used under WSL2 and Windows 10.","title":"Tested distributions under WSL2","type":"posts"},{"content":"","date":"30 January 2021","externalUrl":null,"permalink":"/tags/windows/","section":"Tags","summary":"","title":"Windows","type":"tags"},{"content":"","date":"30 January 2021","externalUrl":null,"permalink":"/categories/wsl/","section":"Categories","summary":"","title":"WSL","type":"categories"},{"content":"","date":"20 January 2021","externalUrl":null,"permalink":"/tags/hosting/","section":"Tags","summary":"","title":"Hosting","type":"tags"},{"content":"After building and setting up my new computer and subsequent move away from the Apple Mac ecosystem, I was left with an issue that the site I had built was setup using a Mac-only tool and could not be migrated to Windows. After mucking around with different tools on Windows and hacking things together, I have now decided to go platform independent and recreate my website using the ever popular WordPress CMS.\nSetting it up on the Synology was a breeze and after finding a suitable theme for my needs I have now started to get things up and running again, adding the content, etc. So far, it looks pretty ok and this may indeed be the way to go.\n","date":"20 January 2021","externalUrl":null,"permalink":"/posts/new-site/","section":"Articles","summary":"After building and setting up my new computer and subsequent move away from the Apple Mac ecosystem, I was left with an issue that the site I had built was setup using a Mac-only tool and could not be migrated to Windows.","title":"New site","type":"posts"},{"content":"","date":"20 January 2021","externalUrl":null,"permalink":"/tags/web-design/","section":"Tags","summary":"","title":"Web Design","type":"tags"},{"content":"","date":"20 January 2021","externalUrl":null,"permalink":"/tags/wordpress/","section":"Tags","summary":"","title":"Wordpress","type":"tags"},{"content":" Introduction # WSL 2 on Windows 10 introduced the ability to run a native linux kernel on your computer while using Windows 10 as your main operating system. Instead of emulating a Linux kernel, like WSL 1 does, WSL 2 uses a lightweight hypervisor to run linux in parallel with Windows.\nTo be able to run WSL 2 on Windows 10, installation of Windows 10 feature update 2004 is required.\nAn explanation on how to enable WSL2 support can be found on the page detailing how to create a Gentoo instance on WSL2.\nAny Linux distribution on Windows? # Yes, you can run any distribution of Linux on WSL quickly and easily, provided that:\nA Docker image for that distribution is available. You install your distribution as normal in a virtual machine first For the Docker method # docker pull image name for your Linux distro docker create --name distro image name for your distro docker export -o distro.tar distro wsl --import \u0026quot;distro\u0026quot; \u0026quot;location for your wsl distro\u0026quot; \u0026quot;PATH/TO/distro.tar\u0026quot; --version 2 How does it work # The first line, the docker pull, downloads the docker image to your computer. This can be any image, as long as it is Linux based, but since you are trying to get a specific Linux distribution for use in WSL, the assumption is it is an image for a Linux distro.\nThe second line, the docker create, creates the docker container. It doesn\u0026rsquo;t start it, it just creates it so you don\u0026rsquo;t have to worry about containers all of a sudden taking up resources/\nThe third line, the docker export, dumps the container into a tar archive. This is the file we need for WSL and contains the entire Linux distribution. At this point, you can throw away your docker container if you want as we do not need it anymore.\nLastly, we are importing the tar archive we created from the docker container into WSL. We can now boot our newly created Linux distro of our choice in WSL and use it, modify it and work it like we want to.\nNOTE: This process works in reverse as well! If you want to create and use a docker image from any WSL2 instance you have created you can simply export the WSL distro to a tar archive and import that into Docker and fire it up!\nFor the virtual machine method # After you have installed the virtual machine, log into the virtual machine and issue the following commands.\n**Note:**this also works for physical Linux machine or dual boot system where you want to copy the Linux system to WSL!\n$ sudo su - # cd / # tar -cpzf backup.tar.gz --exclude=/backup.tar.gz --exclude=/proc --exclude=/tmp --exclude=/mnt --exclude=/dev --exclude=/sys /\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt; This will create a tar archive of the entire system. This can get quite big, especially if you are doing this from a previously installed system and not a clean, base install so make sure you have enough space to save it.\nWhen the tar file has been created, copy it to your Windows machine somehow and issue the following command:\nwsl --import \u0026quot;Your_Distro_Name\u0026quot; \u0026quot;Location_to_store_your_Distro\u0026quot; \u0026quot;PATH/TO/archive.tar\u0026quot; --version 2\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt; This will create the WSL instance by side-loading it and you can start it by issueing wsl.exe -d \u0026lt;Your_Distro_Name\u0026gt; or by opening Windows Terminal; it should already be listed in the dropdown menu as one of the options.\n","date":"19 January 2021","externalUrl":null,"permalink":"/posts/wsl/any-linux-distribution-on-windows-10-with-wsl2/","section":"Articles","summary":"Introduction # WSL 2 on Windows 10 introduced the ability to run a native linux kernel on your computer while using Windows 10 as your main operating system.","title":"Any Linux distribution on Windows 10 with WSL2","type":"posts"},{"content":"","date":"19 January 2021","externalUrl":null,"permalink":"/tags/docker/","section":"Tags","summary":"","title":"Docker","type":"tags"},{"content":"","date":"19 January 2021","externalUrl":null,"permalink":"/tags/gentoo/","section":"Tags","summary":"","title":"Gentoo","type":"tags"},{"content":" Introduction # WSL 2 on Windows 10 introduced the ability to run a native linux kernel on your computer while using Windows 10 as your main operating system. Instead of emulating a linux kernel, like WSL 1 does, WSL 2 uses a hypervisor to run linux in parallel with Windows.\nTo be able to run WSL 2 on Windows 10, installation of Windows 10 feature update 2004 or newer is required.\nEnabling WSL2 support # Open an administrator PowerShell by pressing Windows + X, then A or select PowerShell (Run as Administrator) from the start menu. Execute the following commands to enable the hypervisor to start on system boot.\nEnable-WindowsOptionalFeature -Online -FeatureName Microsoft-Windows-Subsystem-Linux Enable-WindowsOptionalFeature -Online -FeatureName VirtualMachinePlatform Download the latest stage 3 build from the gentoo website (https://gentoo.org/downloads/). Select the profile which suits your needs. In my case I decided to use the most up-to-date, non-hardened, no-multilib, amd64 stage3 build.\nQuick-Link to index: current-stage3-amd64-nomultilib\nnon-hardened: This is only a development system, so no hardening is needed.\nno-multilib: Who needs x86 support anyway when compiling from source?\namd64: Because an up-to-date Windows 10 should be running on an up-to-date amd64 plaform.\nConvert the tar.xz archive into an uncompressed tar archive by opening it with 7-Zip and selecting the extract option from the menu. Or use a Linux distro on WSL already installed. Do not extract the linux filesystem packed in the tar archive.\nOpen an administrator command promt / PowerShell by pressing Windows + X, then A or select PowerShell (Run as Administrator) from the start menu. Run the following command to import the userspace image into your WSL configuration. Be sure that you provide full paths and didn\u0026rsquo;t forget the version argument. The destination path can be adjusted to your liking and is preferably on an SSD with at least 25 GB of free space. The import command will create a single virtual disk image (ext4.vhdx) in the destination folder.\nwsl.exe --import \u0026quot;Gentoo\u0026quot; \u0026quot;D:\\path\\to\\your\\installation\\directory\u0026quot; \u0026quot;C:\\path\\to\\your\\folder\\stage3-amd64-nomultilib-20191113T214501Z.tar\u0026quot; --version 2 If you forgot the version switch, the result of the command above is an extracted linux filesystem in the specified Windows folder. Remove the distribution with wsl unregister and restart the import operation.\nWSL setup in Gentoo # As with every new piece of software, the configuration needs some changes on the WSL / Gentoo side to run smoothly. Run the following commands on the Gentoo instance to fix problems with a non-functional nameserver configuration and file system attributes on Windows drives.\n#!/bin/bash set -e -x # Delete auto-generated files rm /etc/resolv.conf || true rm /etc/wsl.conf || true # Enable changing /etc/resolv.conf # Enable extended attributes on Windows drives cat EOF /etc/wsl.conf [network] generateResolvConf = false [automount] enabled = true options = \u0026quot;metadata\u0026quot; mountFsTab = false EOF # Use google nameservers for DNS resolution cat EOF /etc/resolv.conf nameserver 8.8.8.8 nameserver 8.8.4.4 EOF Create a .wslconfig configuration file for WSL in the Windows user directory. This is necessary to set a maximum size limit of the RAM which WSL may consume. Sometimes the linux kernel uses a portion of the free memory as cache and therefore will start to eat away all RAM of the host system. This can be mitigated by setting the memory configuration option. Replace YOURUSERNAME with your Windows username in the script below.\n#!/bin/bash set -e -x cat EOF /mnt/c/Users/YOURUSERNAME/.wslconf [wsl2] #kernel= memory=8GB #processors= #swap= #swapFile= localhostForwarding=true EOF Now close all instances of Gentoo / WSL and stop the virtual machine by opening the Command Prompt (Windows-Key + X, then A) and issuing the command wsl shutdown.\nGentoo setup in WSL # Run all following commands in the Gentoo instance.\nFor gentoo to work correctly under the hypervisor some feature flags have to be disabled. Open the file /etc/portage/make.conf and adjust the following settings. Below is an example configuration used for my system.\nAdd FEATURES=-ipc-sandbox -pid-sandbox -mount-sandbox -network-sandbox to disbable the non-functional sandboxing features\nAdjust COMMON_FLAGS to match your PC architecture.\nAdjust USE to your needs\nAjdust MAKEOPTS to the number of CPU cores (+1) to make the compilation faster\nThe other options are all optional\n# No GUI (-X -gtk), only english error messages (-nls) USE=\u0026quot;-X -gtk -nls\u0026quot; # Enable python 3.7 and set 3.6 as default PYTHON_TARGETS=\u0026quot;python3_6 python3_7\u0026quot; PYTHON_SINGLE_TARGET=\u0026quot;python3_6\u0026quot; # Define targets for QEMU QEMU_SOFTMMU_TARGETS=\u0026quot;aarch64 arm i386 riscv32 riscv64 x86_64\u0026quot; QEMU_USER_TARGETS=\u0026quot;aarch64 arm i386 riscv32 riscv64 x86_64\u0026quot; # No hardware videocard support VIDEO_CARDS=\u0026quot;dummy\u0026quot; # Disable non-functional sandboxing features FEATURES=\u0026quot;-ipc-sandbox -pid-sandbox -mount-sandbox -network-sandbox\u0026quot; # Always ask when managing packages, always consider deep dependencies (slow) EMERGE_DEFAULT_OPTS=\u0026quot;--ask --complete-graph\u0026quot; # Enable optimizations for the used CPU COMMON_FLAGS=\u0026quot;-march=haswell -O2 -pipe\u0026quot; CHOST=\u0026quot;x86_64-pc-linux-gnu\u0026quot; CFLAGS=\u0026quot;NULL\u0026quot; CXXFLAGS=\u0026quot;NULL\u0026quot; FCFLAGS=\u0026quot;NULL\u0026quot; FFLAGS=\u0026quot;NULL\u0026quot; MAKEOPTS=\u0026quot;-j5\u0026quot; # NOTE: This stage was built with the bindist Use flag enabled PORTDIR=\u0026quot;/var/db/repos/gentoo\u0026quot; DISTDIR=\u0026quot;/var/cache/distfiles\u0026quot; PKGDIR=\u0026quot;/var/cache/binpkgs\u0026quot; # This sets the language of build output to English. # Please keep this setting intact when reporting bugs. LC_MESSAGES=C Final installation steps # To finish the Gentoo installation a new snapshot of the ebuild repository should be downloaded. A recompilation of the compiler ensures that GCC is on the most recent stable version. After updating GCC a recompilation of all programs / libraries ensures that the set optimizations take effect.\n#!/bin/bash set -e -x # Download a snapshot of all official ebuilds emerge-webrsync # Upgrade the compiler and the required libtool library emerge --oneshot --deep sys-devel/gcc emerge --oneshot --usepkg=n sys-devel/libtool # Update all packages with the newly built compiler # This will take a long time, ~1-5 hours emerge --oneshot --emptytree --deep @world emerge --oneshot --deep @preserved-rebuild emerge --ask --depclean Enabling overlays for portage # Eselect provides an easy integration of overlays into portage. The main portage respository should already be configured properly, so only a simple installation of eselect is necessary. For more information see the official wiki. Run the command shown below to install the repository module for eselect.\n#!/bin/bash # Install portage overlays emerge --ask app-eselect/eselect-repository The configuration for the plugin is located in the /etc/eselect/repository.conf file. The default path of the repository index is specified by the REPOS_CONF option which points to /etc/portage/repos.conf by default. Make sure that this directory exists or create it with the following command.\nmkdir -p /etc/portage/repos.conf A file named gentoo.conf should be located in this directory (/etc/portage/repos.conf) which holds the configuration for the main gentoo repository. Below is an example default configuration file which was created on my system.\n[DEFAULT] main-repo = gentoo [gentoo] location = /var/db/repos/gentoo sync-type = rsync sync-uri = rsync://rsync.de.gentoo.org/gentoo-portage/ auto-sync = yes sync-rsync-verify-jobs = 1 sync-rsync-verify-metamanifest = yes sync-rsync-verify-max-age = 24 sync-openpgp-key-path = /usr/share/openpgp-keys/gentoo-release.asc sync-openpgp-keyserver = hkps://keys.gentoo.org sync-openpgp-key-refresh-retry-count = 40 sync-openpgp-key-refresh-retry-overall-timeout = 1200 sync-openpgp-key-refresh-retry-delay-exp-base = 2 sync-openpgp-key-refresh-retry-delay-max = 60 sync-openpgp-key-refresh-retry-delay-mult = 4 sync-webrsync-verify-signature = yes The synchronization of emerge can now be done via the command below. This might take some time, depending on the internet speed and if the repository was synchronized before.\nemerge sync Using Git for portage sync # An alternative method to sync the portage repository is to use git. For this the git package has to be installed on the system as shown below.\n#!/bin/bash # Install the git version control system emerge --ask dev-vcs/git Edit thesync-typeandsync-uriin the portage configuration file under /etc/portage/repos.conf/gentoo.conf as shown below.\n[DEFAULT] main-repo = gentoo [gentoo] location = /var/db/repos/gentoo #sync-type = rsync #sync-uri = rsync://rsync.de.gentoo.org/gentoo-portage/ sync-type = git sync-uri = https://github.com/gentoo-mirror/gentoo.git auto-sync = yes sync-rsync-verify-jobs = 1 sync-rsync-verify-metamanifest = yes sync-rsync-verify-max-age = 24 sync-openpgp-key-path = /usr/share/openpgp-keys/gentoo-release.asc sync-openpgp-keyserver = hkps://keys.gentoo.org sync-openpgp-key-refresh-retry-count = 40 sync-openpgp-key-refresh-retry-overall-timeout = 1200 sync-openpgp-key-refresh-retry-delay-exp-base = 2 sync-openpgp-key-refresh-retry-delay-max = 60 sync-openpgp-key-refresh-retry-delay-mult = 4 sync-webrsync-verify-signature = yes Portage will now complain when synchronizing with git for the first time that the repository folder is not empty and that the folder can not be used to clone the git repository into it. This can be fixed by deleting the old rsync-managed repository which is located under /var/db/repos/gentoo when using the default configuration. Use the command below to do this.\n#!/bin/bash # Remove old and generate new repository rm -r /var/db/repos/gentoo emerge --sync # Sync twice to test synchronization speed emerge --sync\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt; **Final bits and bobs and clean-up** Now that we have Gentoo installed and working in WSL2, we should do some cleaning up and do some things to make our lives easier. First, it would a good idea right now to create and export of our installed Gentoo WSL distribution. wsl.exe --export Gentoo gentoo.tar Store this file somewhere safe. You can use this later to import the Gentoo again if needed, or use it on another computer without having to go through all the steps above.\nNow would also be a good time to add a user to Gentoo that you would normally use, instead of using root. If you are installing Gentoo and have come this far, I probably don\u0026rsquo;t need to explain how to create a user on Linux. So I am not going to.\nAfter you have created the user, note the user ID. Normally, this would be 1000.\nTo set the default user for Gentoo to use, we cannot use the normal procedure as used for distributions installed from the Windows store. Instead, we need to open the registry editor and navigate to:\nHKEY_CURRENT_USER\\SOFTWARE\\Microsoft\\Windows\\CurrentVersion\\Lxss\u0026lt;/code\u0026gt;\u0026lt;/pre\u0026gt; You will see a list of folders with seemingly random names. One of these is for Gentoo, probably the bottom one. Find the one for Gentoo, and edit the key named DefaultUid and set the value to (decimal) 1000 or the number from your user ID if it is something else.\nWARNING: if you do this before you have created your username on Gentoo, the distribution will be unusable, however you can fix it by changing the DefaultUid back to zero. Changes are in effect immediately; no need to reboot after changing this value.\nPRO TIP: If you want to be able to change parameters using the .exe of the distribution as you can when you are using for instance Ubuntu or Debian from the store, grab the .exe from one of these store distro\u0026rsquo;s and put it in the directory where you installed Gentoo. Then rename the file to the name of the distribution as you have added it to WSL. You can now start your distribution using that .exe and for instance set the default user the normal way.\n","date":"19 January 2021","externalUrl":null,"permalink":"/posts/wsl/gentoo-on-wsl/","section":"Articles","summary":"Introduction # WSL 2 on Windows 10 introduced the ability to run a native linux kernel on your computer while using Windows 10 as your main operating system.","title":"Gentoo on Windows with WSL2","type":"posts"},{"content":"","externalUrl":null,"permalink":"/posts/","section":"Articles","summary":"","title":"Articles","type":"posts"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/tags/index/","section":"Tags","summary":"","title":"Index","type":"tags"},{"content":"","externalUrl":null,"permalink":"/posts/linux/","section":"Articles","summary":"","title":"Linux","type":"posts"},{"content":"","externalUrl":null,"permalink":"/posts/opensuse/","section":"Articles","summary":"","title":"OpenSUSE","type":"posts"},{"content":"","externalUrl":null,"permalink":"/posts/quicky/","section":"Articles","summary":"","title":"Quickie","type":"posts"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"},{"content":"","externalUrl":null,"permalink":"/posts/stuff/","section":"Articles","summary":"","title":"Various Stuff","type":"posts"},{"content":"","externalUrl":null,"permalink":"/posts/wsl/","section":"Articles","summary":"","title":"WSL","type":"posts"}]